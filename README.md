# 智联招聘上市公司职位爬虫

## 项目说明

这是一个用于爬取智联招聘上市公司最新职位信息的爬虫程序，使用 Selenium + Chrome 浏览器自动化技术实现。
需要登陆账号，模拟用户点击界面爬取。速度比较慢，大概1000条/4h。
具体爬取后的数据格式可以看项目中的文件。

## 功能特点

- 手动登录方式，避免账号密码硬编码风险
- 自动翻页爬取多页职位信息
- 支持进入详情页获取完整信息
- 随机延迟，避免触发反爬虫机制
- 完善的日志记录和异常处理
- 自动保存为 CSV 格式

## 环境要求

- Python 3.11.4
- Chrome 浏览器（最新版本）
- ChromeDriver（自动下载）

## 安装步骤

### 1. 安装依赖包

在项目目录下打开命令行，执行：

```bash
pip install -r requirements.txt
```

或者单独安装：

```bash
pip install selenium webdriver-manager
```

### 2. 检查 Chrome 浏览器

确保已安装 Chrome 浏览器（推荐最新版本）。

如果未安装，请访问：https://www.google.cn/chrome/

## 使用方法

### 1. 运行爬虫

在项目目录下执行：

```bash
python zhaopin_crawler.py
```

### 2. 手动登录步骤

程序启动后会自动打开浏览器并访问智联招聘网站，请按以下步骤操作：

1. **等待浏览器打开**
   - 程序会自动打开 Chrome 浏览器
   - 会自动访问智联招聘职位列表页面

2. **完成登录**
   - 如果页面显示未登录，请点击"登录"按钮
   - 使用您的智联招聘账号密码登录
   - 如果有验证码，请手动完成验证

3. **确认页面状态**
   - 登录成功后，确保页面显示的是职位列表
   - 检查职位信息是否正常显示
   - 程序会打开两个标签页，一个是职位列表，一个是具体招聘信息的详细数据，这样比较方便翻页，不要误触关闭了。
   
4. **开始爬取**
   - 回到命令行窗口，按 **Enter** 键
   - 程序将自动开始爬取职位数据

5. **等待完成**
   - 程序会自动翻页、点击职位、获取详情
   - 爬取过程中可以看到实时日志输出
   - 完成后会自动保存 CSV 文件
   - 在命令行窗口按Ctrl+C可以停止运行

## 爬取字段

程序会自动爬取以下职位信息：

- 职位名称
- 薪资
- 工作地点
- 公司名称
- 任职要求
- 发布时间
- 招聘人数

## 输出文件

### CSV 文件

数据每8页（160条）会自动保存为 CSV 格式，文件名格式：

```
zhaopin_jobs_page_YYYYMMDD_HHMMSS.csv
```
例如：`zhaopin_jobs_page8_20260104_120000.csv`

### 日志文件

程序运行日志会保存到：

```
zhaopin_crawler.log
```

同时也会在命令行实时显示日志。

## 配置选项

### 修改爬取页数

打开 `zhaopin_crawler.py` 文件，找到 `main()` 函数：

```python
max_pages = None  # 爬取所有页面
# 改为：
max_pages = 5  # 只爬取 5 页
```

### 修改延迟时间

找到 `ZhaopinCrawler` 类的 `__init__` 方法：

```python
self.page_delay_range = (2, 4)  # 随机延迟 2-4 秒
# 可以改为：
self.page_delay_range = (3, 6)  # 随机延迟 3-6 秒（更安全但更慢）
```

### 修改目标 URL

找到 `main()` 函数中的 `target_url`：

```python
target_url = "https://www.zhaopin.com/sou/jl489/p1?ct=9"
```

可以修改为其他职位搜索页面的 URL。

### 修改每次保存的页数
找到 `main()` 函数中的 `save_interval`：

```python
save_interval = 8
```

可以改为任意整数

## 注意事项

### 1. 合法合规

- 本程序仅用于学术研究目的
- 请遵守智联招聘的使用条款
- 请勿过度爬取，避免对网站造成压力
- 爬取的数据仅限个人研究使用，不得商业用途

### 2. 反爬虫应对

- 程序已加入随机延迟机制
- 使用真实浏览器 User-Agent
- 如果遇到验证码，请手动完成
- 如果被封IP，请暂停一段时间再试

### 3. 数据质量

- 网站页面结构可能变化，导致选择器失效
- 如遇到解析错误，请检查日志文件
- 部分职位可能缺少某些字段信息

## 常见问题

### Q1: ChromeDriver 版本不匹配怎么办？

A: 使用 webdriver-manager 会自动下载匹配的 ChromeDriver，无需手动管理。

### Q2: 登录后出现验证码怎么办？

A: 请手动完成验证码验证，程序会等待您完成。

### Q3: 爬取速度太慢怎么办？

A: 可以减小延迟时间，但要注意可能触发反爬虫机制：

```python
self.page_delay_range = (1, 2)  # 加快速度
```

### Q4: 程序中断后数据会丢失吗？

A: 不会，程序会在中断前自动保存已爬取的数据到 CSV 文件。

### Q5: 如何只爬取特定公司的职位？

A: 可以修改目标 URL，添加搜索条件。例如：

```
https://www.zhaopin.com/sou/jl489/p1?ka=search-list_company_123456
```

### Q6: 遇到"多次访问"安全验证怎么办？

**原因分析：**
- 智联招聘检测到自动化访问行为
- 访问频率过快被识别为爬虫
- Selenium 的自动化特征被网站识别

**解决方案：**

1. **手动完成验证**
   - 程序会自动检测到安全验证页面
   - 在浏览器中手动完成滑动验证或其他验证
   - 验证完成后在控制台按 Enter 键继续

2. **降低访问频率**
   - 程序已设置较长的随机延迟（3-6秒）
   - 不要频繁刷新页面
   - 不要同时运行多个爬虫实例

3. **使用真实用户数据**
   - 程序会使用 Chrome 用户数据目录
   - 首次运行后，Chrome 会保存您的登录状态
   - 后续运行无需重新登录

4. **更换网络环境**
   - 如果IP被限制，尝试更换网络
   - 等待一段时间后再试


## 文件说明

```
智联招聘/
├── zhaopin_crawler.py      # 主爬虫程序
├── requirements.txt        # 依赖包列表
└── README.md               # 使用说明文档
```

运行后会生成的文件：

```
├── zhaopin_jobs_pages_YYYYMMDD_HHMMSS.csv  # 爬取的数据文件
└── zhaopin_crawler.log               # 日志文件
```

## 技术支持

如遇到问题，请查看：

1. 日志文件 `zhaopin_crawler.log`
2. 命令行的错误信息
3. 检查网络连接是否正常
4. 确认 Chrome 浏览器是否正常

## 免责声明

本程序仅供学习和研究使用。使用者需自行承担使用本程序产生的一切后果，包括但不限于法律风险。开发者不对因使用本程序造成的任何损失负责。
本README由AI生成，部分解释可能存在问题，请根据具体实际情况具体分析。
笨人根本不会爬虫。本代码是在AI生成的代码基础上，根据网页结构修改了需要获取数据的位置，然后成功运行的，供大家参考。
